{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6135437",
   "metadata": {},
   "source": [
    "#### Carregamento de bibliotecas e DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "296b1c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        age       sex       bmi        bp        s1        s2        s3  \\\n",
      "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
      "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
      "2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
      "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
      "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
      "\n",
      "         s4        s5        s6  target  \n",
      "0 -0.002592  0.019907 -0.017646   151.0  \n",
      "1 -0.039493 -0.068332 -0.092204    75.0  \n",
      "2 -0.002592  0.002861 -0.025930   141.0  \n",
      "3  0.034309  0.022688 -0.009362   206.0  \n",
      "4 -0.002592 -0.031988 -0.046641   135.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "#Carrega o dataset\n",
    "df = pd.read_csv(r'.\\diabetes_dataset.csv', sep=',')\n",
    "df_polars = pl.read_csv(r'.\\diabetes_dataset.csv', separator=',')  \n",
    "\n",
    "# Verifica se o dataframe foi carregado corretamente\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8e6740",
   "metadata": {},
   "source": [
    "#### Exercício 1 – Deteção de Outliers Pandas (IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "efcae860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercicio 1 - Número de outliers detectados: {'bmi': 3, 'bp': 0, 's5': 4}\n"
     ]
    }
   ],
   "source": [
    "def detectar_outliers_iqr(df, cols):\n",
    "    outliers_info = {}\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            limite_inferior = Q1 - 1.5 * IQR\n",
    "            limite_superior = Q3 + 1.5 * IQR\n",
    "            outliers = df[(df[col] < limite_inferior) | (df[col] > limite_superior)]\n",
    "            outliers_info[col] = len(outliers)\n",
    "            \n",
    "    return outliers_info\n",
    "\n",
    "outliers_ex1 = detectar_outliers_iqr(df, ['bmi', 'bp', 's5'])\n",
    "\n",
    "print('Exercicio 1 - Número de outliers detectados:', outliers_ex1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e35d98b",
   "metadata": {},
   "source": [
    "#### Exercício 2 – Capping de Valores Extremos Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "12d140f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercicio 2 - Capping aplicado\n"
     ]
    }
   ],
   "source": [
    "def aplicar_capping(df, cols):\n",
    "    capped_df = df.copy()\n",
    "    for col in cols:\n",
    "        p5 = df[col].quantile(0.05)\n",
    "        p95 = df[col].quantile(0.95)\n",
    "        capped_df[col] = capped_df[col].clip(lower=p5, upper=p95)\n",
    "    return capped_df\n",
    "\n",
    "df_capped = aplicar_capping(df, ['bmi', 'bp', 's5'])\n",
    "\n",
    "print('Exercicio 2 - Capping aplicado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3e15f8",
   "metadata": {},
   "source": [
    "#### Exercício 3 – Duplicados e Valores Nulos (simulados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9a5b4b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercício 3 - Duplicados antes: 1\n",
      "Exercício 3 - Duplicados depois: 0\n",
      "\n",
      "Exercicio 3 - Nulos e duplicados removidos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_29452\\503058056.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df3['bmi'].fillna(df3['bmi'].median(), inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_29452\\503058056.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df3['bp'].fillna(df3['bp'].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df3 = df.copy()\n",
    "df3.loc[0, 'bmi'] = None\n",
    "df3.loc[1, 'bp'] = None\n",
    "df3 = pd.concat([df3, df3.iloc[[2]]]) \n",
    "\n",
    "duplicados_ex3 = df3.duplicated().sum()\n",
    "print('Exercício 3 - Duplicados antes:', duplicados_ex3)\n",
    "\n",
    "##remove duplicados\n",
    "df3 = df3.drop_duplicates()\n",
    "df3['bmi'].fillna(df3['bmi'].median(), inplace=True)\n",
    "df3['bp'].fillna(df3['bp'].median(), inplace=True)\n",
    "\n",
    "df_depois = df3.duplicated().sum()\n",
    "print('Exercício 3 - Duplicados depois:', df_depois)\n",
    "\n",
    "print('\\nExercicio 3 - Nulos e duplicados removidos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125b89e5",
   "metadata": {},
   "source": [
    "#### Exercício 4 – Encoding de Variáveis Categóricas (simuladas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d168af05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercicio 4 - Codificação categórica concluída\n"
     ]
    }
   ],
   "source": [
    "df4 = df.copy()\n",
    "df4['categoria_idade'] = pd.cut(df4['age'], bins=[0, 0.3, 0.6, 1.0], labels=['jovem', 'adulto', 'sénior'])\n",
    "df4_label_encoded = df4.copy()\n",
    "df4_label_encoded['categoria_idade'] = df4_label_encoded['categoria_idade'].astype('category').cat.codes\n",
    "df4_onehot_encoded = pd.get_dummies(df4, columns=['categoria_idade'], drop_first=True)\n",
    "print('Exercicio 4 - Codificação categórica concluída')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f8ac84",
   "metadata": {},
   "source": [
    "#### Exercício 5 – Escalonamento de Variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5adcf2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercicio 5 - Escalonamento aplicado\n"
     ]
    }
   ],
   "source": [
    "df5 = df.copy()\n",
    "scaler = StandardScaler()\n",
    "df5_scaled = df5.copy()\n",
    "df5_scaled[df5.columns] = scaler.fit_transform(df5[df5.columns])\n",
    "\n",
    "print('Exercicio 5 - Escalonamento aplicado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd85234d",
   "metadata": {},
   "source": [
    "#### Exercício 6 – Feature Engineering + Seleção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "be6248ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercício 6 - Features selecionadas: ['age', 'bmi', 'bp', 's4', 's5']\n"
     ]
    }
   ],
   "source": [
    "df6 = df.copy()\n",
    "df6['bmi_bp_interaction'] = df6['bmi'] * df6['bp']\n",
    "#print(df6.head())\n",
    "\n",
    "X = df6.drop(columns=['s6']) \n",
    "y = (df6['s6'] > df6['s6'].median()).astype(int)\n",
    "\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "selector = SelectKBest(score_func=f_classif, k=5)\n",
    "X_selected = selector.fit_transform(X_scaled, y)\n",
    "selected_features = X.columns[selector.get_support()].tolist()\n",
    "\n",
    "print('Exercício 6 - Features selecionadas:', selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba29c4",
   "metadata": {},
   "source": [
    "## Polars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef086f58",
   "metadata": {},
   "source": [
    "#### Exercício 1 – Deteção de Outliers Polars (IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6efbf7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercicio 1 - Outliers encontrados: {'bmi': 3, 'bp': 0, 's5': 4}\n"
     ]
    }
   ],
   "source": [
    "def detectar_outliers_iqr(df, cols):\n",
    "    outliers_info = {}\n",
    "    for col in cols:\n",
    "        q1 = df.select(pl.col(col).quantile(0.25)).item()\n",
    "        q3 = df.select(pl.col(col).quantile(0.75)).item()\n",
    "        iqr = q3 - q1\n",
    "        limite_inferior = q1 - 1.5 * iqr\n",
    "        limite_superior = q3 + 1.5 * iqr\n",
    "        outliers = df.filter((pl.col(col) < limite_inferior) | (pl.col(col) > limite_superior))\n",
    "        outliers_info[col] = outliers.height\n",
    "    return outliers_info\n",
    "\n",
    "outliers_ex1 = detectar_outliers_iqr(df_polars, ['bmi', 'bp', 's5'])\n",
    "print('Exercicio 1 - Outliers encontrados:', outliers_ex1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248c871b",
   "metadata": {},
   "source": [
    "#### Exercício 2 – Capping de Valores Extremos Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ec6424c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercício 2 - Capping aplicado\n"
     ]
    }
   ],
   "source": [
    "def aplicar_capping(df, cols):\n",
    "    df_capped = df.clone()\n",
    "    for col in cols:\n",
    "        p5 = df.select(pl.col(col).quantile(0.05)).item()\n",
    "        p95 = df.select(pl.col(col).quantile(0.95)).item()\n",
    "        df_capped = df_capped.with_columns(\n",
    "            pl.col(col).clip(lower_bound=p5, upper_bound=p95).alias(col)\n",
    "        )\n",
    "    return df_capped\n",
    "\n",
    "df_capped = aplicar_capping(df_polars, ['bmi', 'bp', 's5'])\n",
    "print('Exercício 2 - Capping aplicado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b2dae",
   "metadata": {},
   "source": [
    "#### Exercício 3 – Duplicados e Valores Nulos (simulados) Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e812c40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercício 3 - Duplicados antes: 2\n",
      "Exercício 3 - Duplicados depois: 0\n",
      "\n",
      "Exercício 3 - Nulos imputados e duplicados removidos\n"
     ]
    }
   ],
   "source": [
    "df3 = df_polars.clone()\n",
    "df3 = df3.with_columns([\n",
    "    pl.when(pl.arange(0, df3.height) == 0).then(None).otherwise(pl.col('bmi')).alias('bmi'),\n",
    "    pl.when(pl.arange(0, df3.height) == 1).then(None).otherwise(pl.col('bp')).alias('bp')\n",
    "])\n",
    "\n",
    "df3 = df3.vstack(df3[2:3]) \n",
    "\n",
    "duplicados_ex3 = df3.is_duplicated().sum()\n",
    "print('Exercício 3 - Duplicados antes:', duplicados_ex3)\n",
    "\n",
    "# remove duplicados\n",
    "df3 = df3.unique()\n",
    "df3 = df3.with_columns([\n",
    "    pl.col('bmi').fill_null(pl.col('bmi').median()),\n",
    "    pl.col('bp').fill_null(pl.col('bp').median())\n",
    "])\n",
    "duplicados_depois = df3.is_duplicated().sum()\n",
    "print('Exercício 3 - Duplicados depois:', duplicados_depois)\n",
    "\n",
    "print('\\nExercício 3 - Nulos imputados e duplicados removidos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80de4762",
   "metadata": {},
   "source": [
    "#### Exercício 4 – Encoding de Variáveis Categóricas (simuladas) Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7a288f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercício 4 - Codificação categórica concluída\n"
     ]
    }
   ],
   "source": [
    "df4 = df_polars.clone()\n",
    "df4 = df4.with_columns([\n",
    "    pl.when(pl.col('age') <= 0.3).then(pl.lit('jovem'))\n",
    "    .when(pl.col('age') <= 0.6).then(pl.lit('adulto'))\n",
    "    .otherwise(pl.lit('sénior'))\n",
    "    .alias('categoria_idade')\n",
    "])\n",
    "\n",
    "# Label Encoding\n",
    "df4_label_encoded = df4.with_columns([\n",
    "    pl.col('categoria_idade').cast(pl.Categorical)\n",
    "])\n",
    "\n",
    "# One-Hot Encoding\n",
    "df4_label_encoded = df4.to_dummies(columns=['categoria_idade'])\n",
    "print('Exercício 4 - Codificação categórica concluída')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b5940d",
   "metadata": {},
   "source": [
    "#### Exercício 5 – Escalonamento de Variáveis Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "55f25311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 11)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ age       ┆ sex       ┆ bmi       ┆ bp        ┆ … ┆ s4        ┆ s5        ┆ s6        ┆ target   │\n",
      "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
      "│ f64       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 0.8005    ┆ 1.065488  ┆ 1.297088  ┆ 0.459841  ┆ … ┆ -0.054499 ┆ 0.418531  ┆ -0.370989 ┆ -0.01471 │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 9        │\n",
      "│ -0.039567 ┆ -0.938537 ┆ -1.08218  ┆ -0.553505 ┆ … ┆ -0.830301 ┆ -1.436589 ┆ -1.938479 ┆ -1.00165 │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 9        │\n",
      "│ 1.793307  ┆ 1.065488  ┆ 0.934533  ┆ -0.119214 ┆ … ┆ -0.054499 ┆ 0.060156  ┆ -0.545154 ┆ -0.14458 │\n",
      "│ -1.872441 ┆ -0.938537 ┆ -0.243771 ┆ -0.77065  ┆ … ┆ 0.721302  ┆ 0.476983  ┆ -0.196823 ┆ 0.699513 │\n",
      "│ 0.113172  ┆ -0.938537 ┆ -0.764944 ┆ 0.459841  ┆ … ┆ -0.054499 ┆ -0.672502 ┆ -0.980568 ┆ -0.22249 │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 6        │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n",
      "Exercício 5 - Escalonamento aplicado\n"
     ]
    }
   ],
   "source": [
    "df5 = df_polars.clone()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df5_scaled_np = scaler.fit_transform(df5.to_numpy())\n",
    "df5_scaled = pl.DataFrame(df5_scaled_np, schema=df_polars.columns)\n",
    "print(df5_scaled.head())\n",
    "print('Exercício 5 - Escalonamento aplicado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8275b55",
   "metadata": {},
   "source": [
    "#### Exercício 6 – Feature Engineering + Seleção (Polars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a541a065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercício 6 - Features selecionadas: ['age', 'bmi', 'bp', 's4', 's5']\n"
     ]
    }
   ],
   "source": [
    "df6 = df_polars.clone()\n",
    "df6 = df6.with_columns([\n",
    "    (pl.col('bmi') * pl.col('bp')).alias('bmi_bp_interaction')\n",
    "])\n",
    "\n",
    "X = df6.drop('s6').to_pandas()\n",
    "y = (df6['s6'].to_numpy() > df6['s6'].median()).astype(int)\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "selector = SelectKBest(score_func=f_classif, k=5)\n",
    "X_selected = selector.fit_transform(X_scaled, y)\n",
    "selected_features = X.columns[selector.get_support()].tolist()\n",
    "\n",
    "print('Exercício 6 - Features selecionadas:', selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9473d380",
   "metadata": {},
   "source": [
    "### Exercício 7 em PANDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e45d0f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicados antes: 0\n",
      "Duplicados depois: 0\n",
      "\n",
      "Dataset final pronto para modelos.\n",
      "     Row ID  Postal Code     Sales  Ship Mode_Same Day  \\\n",
      "0 -1.731874    -0.401493  0.049776               False   \n",
      "1 -1.731521    -0.401493  0.799801               False   \n",
      "2 -1.731167     1.085497 -0.344944               False   \n",
      "3 -1.730814    -0.685956  1.159887               False   \n",
      "4 -1.730460    -0.685956 -0.332580               False   \n",
      "\n",
      "   Ship Mode_Second Class  Ship Mode_Standard Class  Segment_Corporate  \\\n",
      "0                    True                     False              False   \n",
      "1                    True                     False              False   \n",
      "2                    True                     False               True   \n",
      "3                   False                      True              False   \n",
      "4                   False                      True              False   \n",
      "\n",
      "   Segment_Home Office  City_Abilene  City_Akron  ...  Sub-Category_Envelopes  \\\n",
      "0                False         False       False  ...                   False   \n",
      "1                False         False       False  ...                   False   \n",
      "2                False         False       False  ...                   False   \n",
      "3                False         False       False  ...                   False   \n",
      "4                False         False       False  ...                   False   \n",
      "\n",
      "   Sub-Category_Fasteners  Sub-Category_Furnishings  Sub-Category_Labels  \\\n",
      "0                   False                     False                False   \n",
      "1                   False                     False                False   \n",
      "2                   False                     False                 True   \n",
      "3                   False                     False                False   \n",
      "4                   False                     False                False   \n",
      "\n",
      "   Sub-Category_Machines  Sub-Category_Paper  Sub-Category_Phones  \\\n",
      "0                  False               False                False   \n",
      "1                  False               False                False   \n",
      "2                  False               False                False   \n",
      "3                  False               False                False   \n",
      "4                  False               False                False   \n",
      "\n",
      "   Sub-Category_Storage  Sub-Category_Supplies  Sub-Category_Tables  \n",
      "0                 False                  False                False  \n",
      "1                 False                  False                False  \n",
      "2                 False                  False                False  \n",
      "3                 False                  False                 True  \n",
      "4                  True                  False                False  \n",
      "\n",
      "[5 rows x 605 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_29452\\2110225437.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Postal Code'].fillna(df['Postal Code'].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1 Carrega o dataset\n",
    "\n",
    "DATA_PATH = r'.\\train.csv'\n",
    "df = pd.read_csv(DATA_PATH, sep=',')\n",
    "#print(df.head())\n",
    "\n",
    "# 2. Tratamento de valores nulos e duplicados\n",
    "print('Duplicados antes:', df.duplicated().sum())\n",
    "df = df.drop_duplicates()\n",
    "print('Duplicados depois:', df.duplicated().sum())\n",
    "\n",
    "df['Postal Code'].fillna(df['Postal Code'].median(), inplace=True)\n",
    "\n",
    "# 3. Codificação de variáveis categóricas\n",
    "cat_cols = df.select_dtypes(include='object').columns\n",
    "\n",
    "drop_cols = ['Order ID', 'Order Date', 'Ship Date', 'Customer ID', 'Customer Name',\n",
    "'Product ID', 'Product Name']\n",
    "df = df.drop(columns=drop_cols)\n",
    "\n",
    "df_encoded = pd.get_dummies(df, columns=[col for col in cat_cols if col not in drop_cols], drop_first=True)\n",
    "\n",
    "# 4. Escalonamento de variáveis numéricas\n",
    "scaler = StandardScaler()\n",
    "num_cols = df_encoded.select_dtypes(include='number').columns\n",
    "df_encoded[num_cols] = scaler.fit_transform(df_encoded[num_cols])\n",
    "\n",
    "# 5. Dataset pronto para modelos\n",
    "print('\\nDataset final pronto para modelos.')\n",
    "print(df_encoded.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026b4ddf",
   "metadata": {},
   "source": [
    "### Exercício 7 em POLARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6ad2526d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicados antes: 0\n",
      "Duplicados depois: 0\n",
      "\n",
      " Dataset final pronto para modelos.\n",
      "shape: (5, 613)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ Row ID    ┆ Ship Mode ┆ Ship      ┆ Ship Mode ┆ … ┆ Sub-Categ ┆ Sub-Categ ┆ Sub-Categ ┆ Sales    │\n",
      "│ ---       ┆ _First    ┆ Mode_Same ┆ _Second   ┆   ┆ ory_Stora ┆ ory_Suppl ┆ ory_Table ┆ ---      │\n",
      "│ f64       ┆ Class     ┆ Day       ┆ Class     ┆   ┆ ge        ┆ ies       ┆ s         ┆ f64      │\n",
      "│           ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆          │\n",
      "│           ┆ u8        ┆ u8        ┆ u8        ┆   ┆ u8        ┆ u8        ┆ u8        ┆          │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ -1.518019 ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 0         ┆ 0         ┆ 0         ┆ -0.29935 │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 4        │\n",
      "│ 0.767935  ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 0         ┆ 0         ┆ 0         ┆ -0.36000 │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 3        │\n",
      "│ 1.572808  ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 0         ┆ 0         ┆ 0         ┆ -0.30131 │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 4        │\n",
      "│ -0.22499  ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 0         ┆ 0         ┆ 0         ┆ -0.34964 │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 7        │\n",
      "│ 0.580237  ┆ 1         ┆ 0         ┆ 0         ┆ … ┆ 0         ┆ 0         ┆ 0         ┆ -0.14498 │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 3        │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Carregamento do dataset\n",
    "DATA_PATH = r'.\\train.csv'\n",
    "df = pl.read_csv(DATA_PATH, separator=',')\n",
    "\n",
    "# 2. Tratamento de valores nulos e duplicados\n",
    "duplicados_antes = df.is_duplicated().sum()\n",
    "\n",
    "print('Duplicados antes:', duplicados_antes)\n",
    "df = df.unique()\n",
    "print('Duplicados depois:', df.is_duplicated().sum())\n",
    "\n",
    "postal_median = df.select(pl.col('Postal Code').median()).item()\n",
    "df = df.with_columns(\n",
    "            pl.col('Postal Code').fill_null(postal_median)\n",
    "            )\n",
    "\n",
    "# 3. Codificação de variáveis categóricas\n",
    "drop_cols = ['Order ID', 'Order Date', 'Ship Date', 'Customer ID', 'Customer Name','Product ID', 'Product Name']\n",
    "\n",
    "df = df.drop(drop_cols)\n",
    "\n",
    "cat_cols = [col for col, dtype in zip(df.columns, df.dtypes) if dtype == pl.Utf8]\n",
    "\n",
    "df = df.to_dummies(columns=cat_cols)\n",
    "\n",
    "# 4. Escalonamento de variáveis numéricas\n",
    "scaler = StandardScaler()\n",
    "numeric_cols = [col for col, dtype in zip(df.columns, df.dtypes) if dtype in (pl.Int64, pl.Float64)]\n",
    "df_scaled_np = scaler.fit_transform(df[numeric_cols].to_numpy())\n",
    "df_scaled = df.with_columns([\n",
    "pl.Series(name=col, values=df_scaled_np[:, i]) for i, col in enumerate(numeric_cols)\n",
    "])\n",
    "\n",
    "# 5. Dataset pronto para modelos\n",
    "print('\\n Dataset final pronto para modelos.')\n",
    "print(df_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbf7d57",
   "metadata": {},
   "source": [
    "## Desafio Autónomo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0b476eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicados antes: 0\n",
      "Duplicados depois: 0\n",
      "\n",
      " Dimensões do dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 299 entries, 0 to 298\n",
      "Data columns (total 13 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   age                       299 non-null    float64\n",
      " 1   anaemia                   299 non-null    int64  \n",
      " 2   creatinine_phosphokinase  299 non-null    int64  \n",
      " 3   diabetes                  299 non-null    int64  \n",
      " 4   ejection_fraction         299 non-null    int64  \n",
      " 5   high_blood_pressure       299 non-null    int64  \n",
      " 6   platelets                 299 non-null    float64\n",
      " 7   serum_creatinine          299 non-null    float64\n",
      " 8   serum_sodium              299 non-null    int64  \n",
      " 9   sex                       299 non-null    int64  \n",
      " 10  smoking                   299 non-null    int64  \n",
      " 11  time                      299 non-null    int64  \n",
      " 12  DEATH_EVENT               299 non-null    int64  \n",
      "dtypes: float64(3), int64(10)\n",
      "memory usage: 30.5 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# 1. Carregamento do dataset\n",
    "DATA_PATH = r'.\\heart_failure_clinical_records_dataset.csv'\n",
    "df_heart = pd.read_csv(DATA_PATH, sep=',', na_values=[\"N/D\", \"NA\"])\n",
    "#print(df_heart.head())\n",
    "\n",
    "# A. Exploração inicial (EDA breve)\n",
    "#Tratamento de valores nulos e duplicados, dimensoes do dataset\n",
    "print('Duplicados antes:', df_heart.duplicated().sum())\n",
    "df_heart = df_heart.drop_duplicates()\n",
    "print('Duplicados depois:', df_heart.duplicated().sum())\n",
    "\n",
    "print(\"\\n Dimensões do dataset:\")\n",
    "print(df_heart.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6d01210e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outliers antes da limpeza:\n",
      "age: 0 outliers\n",
      "anaemia: 0 outliers\n",
      "creatinine_phosphokinase: 29 outliers\n",
      "diabetes: 0 outliers\n",
      "ejection_fraction: 2 outliers\n",
      "high_blood_pressure: 0 outliers\n",
      "platelets: 21 outliers\n",
      "serum_creatinine: 29 outliers\n",
      "serum_sodium: 4 outliers\n",
      "sex: 0 outliers\n",
      "smoking: 0 outliers\n",
      "time: 0 outliers\n",
      "DEATH_EVENT: 0 outliers\n",
      "\n",
      "Outliers depois da limpeza:\n",
      "age: 0 outliers\n",
      "anaemia: 0 outliers\n",
      "creatinine_phosphokinase: 0 outliers\n",
      "diabetes: 0 outliers\n",
      "ejection_fraction: 0 outliers\n",
      "high_blood_pressure: 0 outliers\n",
      "platelets: 0 outliers\n",
      "serum_creatinine: 3 outliers\n",
      "serum_sodium: 0 outliers\n",
      "sex: 0 outliers\n",
      "smoking: 0 outliers\n",
      "time: 0 outliers\n",
      "DEATH_EVENT: 0 outliers\n"
     ]
    }
   ],
   "source": [
    "# B. Limpeza de outliers\n",
    "# B. Limpeza de outliers\n",
    "def remove_outliers(df, columns):\n",
    "    outliers_count_before = {col: ((df[col] < df[col].quantile(0.25) - 1.5 * (df[col].quantile(0.75) - df[col].quantile(0.25))).sum() +\n",
    "                                    (df[col] > df[col].quantile(0.75) + 1.5 * (df[col].quantile(0.75) - df[col].quantile(0.25))).sum()) for col in columns}\n",
    "    \n",
    "    for col in columns:\n",
    "        IQR = df[col].quantile(0.75) - df[col].quantile(0.25)\n",
    "        lower_bound = df[col].quantile(0.25) - 1.5 * IQR\n",
    "        upper_bound = df[col].quantile(0.75) + 1.5 * IQR\n",
    "        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "    \n",
    "    outliers_count_after = {col: ((df[col] < df[col].quantile(0.25) - 1.5 * (df[col].quantile(0.75) - df[col].quantile(0.25))).sum() +\n",
    "                                   (df[col] > df[col].quantile(0.75) + 1.5 * (df[col].quantile(0.75) - df[col].quantile(0.25))).sum()) for col in columns}\n",
    "    \n",
    "    return df, outliers_count_before, outliers_count_after\n",
    "\n",
    "numeric_columns = df_heart.select_dtypes(include=[np.number]).columns\n",
    "df_heart_cleaned, outliers_before, outliers_after = remove_outliers(df_heart, numeric_columns)\n",
    "\n",
    "print(\"\\nOutliers antes da limpeza:\")\n",
    "for col, count in outliers_before.items():\n",
    "    print(f\"{col}: {count} outliers\")\n",
    "    \n",
    "print(\"\\nOutliers depois da limpeza:\")\n",
    "for col, count in outliers_after.items():\n",
    "    print(f\"{col}: {count} outliers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3049069f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contagem de nulos\n",
      "age                         0\n",
      "anaemia                     0\n",
      "creatinine_phosphokinase    0\n",
      "diabetes                    0\n",
      "ejection_fraction           0\n",
      "high_blood_pressure         0\n",
      "platelets                   0\n",
      "serum_creatinine            0\n",
      "serum_sodium                0\n",
      "sex                         0\n",
      "smoking                     0\n",
      "time                        0\n",
      "DEATH_EVENT                 0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_29452\\3475200205.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_heart_cleaned[col].fillna(df_heart_cleaned[col].median(), inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_29452\\3475200205.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_heart_cleaned[col].fillna(df_heart_cleaned[col].median(), inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_29452\\3475200205.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_heart_cleaned[col].fillna(df_heart_cleaned[col].median(), inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_29452\\3475200205.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_heart_cleaned[col].fillna(df_heart_cleaned[col].median(), inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_29452\\3475200205.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_heart_cleaned[col].fillna(df_heart_cleaned[col].median(), inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_29452\\3475200205.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_heart_cleaned[col].fillna(df_heart_cleaned[col].median(), inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_29452\\3475200205.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_heart_cleaned[col].fillna(df_heart_cleaned[col].median(), inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_29452\\3475200205.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_heart_cleaned[col].fillna(df_heart_cleaned[col].median(), inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_29452\\3475200205.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_heart_cleaned[col].fillna(df_heart_cleaned[col].median(), inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_29452\\3475200205.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_heart_cleaned[col].fillna(df_heart_cleaned[col].median(), inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_29452\\3475200205.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_heart_cleaned[col].fillna(df_heart_cleaned[col].median(), inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_29452\\3475200205.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_heart_cleaned[col].fillna(df_heart_cleaned[col].median(), inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_29452\\3475200205.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_heart_cleaned[col].fillna(df_heart_cleaned[col].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# C. Tratamento de valores em falta\n",
    "    \n",
    "\n",
    "# Imputação de valores em falta\n",
    "# Preencher variáveis numéricas, usamos a mediana\n",
    "for col in df_heart_cleaned.select_dtypes(include=[np.number]).columns:\n",
    "    df_heart_cleaned[col].fillna(df_heart_cleaned[col].median(), inplace=True)\n",
    "    \n",
    "\n",
    "#Imputação de valores em falta para variáveis categóricas com moda\n",
    "cat_cols = df_heart_cleaned.select_dtypes(include=[object]).columns\n",
    "for col in cat_cols:\n",
    "    moda = df_heart_cleaned[col].mode()[0]\n",
    "    df_heart_cleaned[col].fillna(moda, inplace=True)\n",
    "\n",
    "print('\\nContagem de nulos')\n",
    "print(df_heart_cleaned.isnull().sum()) #verifica se existem valores nulos\n",
    "df_heart_cleaned = df_heart_cleaned.dropna() #remove os valores nulos\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c9f93186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribuição de idades:\n",
      "categoria_idade\n",
      "60-79         145\n",
      "40-59         129\n",
      "80 ou mais     25\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribuição de Serum:\n",
      "categoria_serum\n",
      "Bom        205\n",
      "Baixo       83\n",
      "Elevado     11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribuição de Sexo:\n",
      "categoria_sexo\n",
      "M    194\n",
      "F    105\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#D. Engenharia de features\n",
    "def categorizar_idade(idade):\n",
    "    if idade < 40:\n",
    "        return 'Menos de 40'\n",
    "    elif 40 <= idade < 60:\n",
    "        return '40-59'\n",
    "    elif 60 <= idade < 80:\n",
    "        return '60-79'\n",
    "    else:\n",
    "        return '80 ou mais'\n",
    "    \n",
    "#recurso a função\n",
    "df_heart['categoria_idade'] = df_heart['age'].apply(categorizar_idade)\n",
    "# recurso a numpy where\n",
    "df_heart['categoria_sexo'] = np.where(df_heart['sex'] > 0, 'M', 'F')\n",
    "# recurso a lambda\n",
    "df_heart['categoria_serum'] = df_heart['serum_sodium'].apply(lambda x: 'Baixo' if x < 135 else 'Bom' if x < 145 else 'Elevado' )\n",
    "\n",
    "print ('\\nDistribuição de idades:')\n",
    "print(df_heart['categoria_idade'].value_counts())\n",
    "print ('\\nDistribuição de Serum:')\n",
    "print(df_heart['categoria_serum'].value_counts())\n",
    "print ('\\nDistribuição de Sexo:')\n",
    "print(df_heart['categoria_sexo'].value_counts())\n",
    "#print(df_heart.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "03ee91f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoding:     age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
      "0  75.0        0                       582         0                 20   \n",
      "2  65.0        0                       146         0                 20   \n",
      "3  50.0        1                       111         0                 20   \n",
      "5  90.0        1                        47         0                 40   \n",
      "6  75.0        1                       246         0                 15   \n",
      "\n",
      "   high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n",
      "0                    1   265000.0               1.9           130    1   \n",
      "2                    0   162000.0               1.3           129    1   \n",
      "3                    0   210000.0               1.9           137    1   \n",
      "5                    1   204000.0               2.1           132    1   \n",
      "6                    0   127000.0               1.2           137    1   \n",
      "\n",
      "   smoking  time  DEATH_EVENT  \n",
      "0        0     4            1  \n",
      "2        1     7            1  \n",
      "3        0     7            1  \n",
      "5        1     8            1  \n",
      "6        0    10            1  \n",
      "\n",
      "Label Encoding:     age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
      "0  75.0        0                       582         0                 20   \n",
      "2  65.0        0                       146         0                 20   \n",
      "3  50.0        1                       111         0                 20   \n",
      "5  90.0        1                        47         0                 40   \n",
      "6  75.0        1                       246         0                 15   \n",
      "\n",
      "   high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n",
      "0                    1   265000.0               1.9           130    1   \n",
      "2                    0   162000.0               1.3           129    1   \n",
      "3                    0   210000.0               1.9           137    1   \n",
      "5                    1   204000.0               2.1           132    1   \n",
      "6                    0   127000.0               1.2           137    1   \n",
      "\n",
      "   smoking  time  DEATH_EVENT  \n",
      "0        0     4            1  \n",
      "2        1     7            1  \n",
      "3        0     7            1  \n",
      "5        1     8            1  \n",
      "6        0    10            1  \n"
     ]
    }
   ],
   "source": [
    "#E. Codificação de variáveis categóricas\n",
    "\n",
    "cat_cols = df_heart_cleaned.select_dtypes(include=[object]).columns\n",
    "\n",
    "# Aplicar One-Hot Encoding \n",
    "df_one_hot = pd.get_dummies(df_heart_cleaned, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# Label Encoding para variáveis categóricas\n",
    "label_encoder = LabelEncoder()  #Inicializando o LabelEncoder\n",
    "df_label_encoded = df_heart_cleaned.copy()\n",
    "\n",
    "for col in cat_cols:\n",
    "    df_label_encoded[col] = label_encoder.fit_transform(df_label_encoded[col])\n",
    "\n",
    "# Verificar resultados\n",
    "print(\"One-Hot Encoding:\", df_one_hot.head())\n",
    "print(\"\\nLabel Encoding:\", df_label_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7a3e32bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipos de dados antes da normalização:\n",
      "age                         float64\n",
      "anaemia                       int64\n",
      "creatinine_phosphokinase      int64\n",
      "diabetes                      int64\n",
      "ejection_fraction             int64\n",
      "high_blood_pressure           int64\n",
      "platelets                   float64\n",
      "serum_creatinine            float64\n",
      "serum_sodium                  int64\n",
      "sex                           int64\n",
      "smoking                       int64\n",
      "time                          int64\n",
      "DEATH_EVENT                   int64\n",
      "dtype: object\n",
      "Tipos de dados depois da normalização:\n",
      "age                         float64\n",
      "anaemia                     float64\n",
      "creatinine_phosphokinase    float64\n",
      "diabetes                    float64\n",
      "ejection_fraction           float64\n",
      "high_blood_pressure         float64\n",
      "platelets                   float64\n",
      "serum_creatinine            float64\n",
      "serum_sodium                float64\n",
      "sex                         float64\n",
      "smoking                     float64\n",
      "time                        float64\n",
      "DEATH_EVENT                 float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#F. Escalonamento / normalização\n",
    "\n",
    "# Verificar tipos de dados antes da normalização\n",
    "print(f'Tipos de dados antes da normalização:\\n{df_heart_cleaned.dtypes}')\n",
    "\n",
    "\n",
    "# Normalizar todas as variáveis numéricas\n",
    "numeric_columns = df_heart_cleaned.select_dtypes(include=[np.number]).columns\n",
    "scaler = StandardScaler() # Inicializando o StandardScaler\n",
    "df_heart_cleaned[numeric_columns] = scaler.fit_transform(df_heart_cleaned[numeric_columns])\n",
    "\n",
    "print(f'Tipos de dados depois da normalização:\\n{df_heart_cleaned.dtypes}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7e15fbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribuição inicial:\n",
      "               age       anaemia  creatinine_phosphokinase      diabetes  \\\n",
      "mean  2.220446e-16  5.749369e-17              4.758099e-17 -6.344132e-17   \n",
      "min  -1.745855e+00 -9.393364e-01             -1.100559e+00 -8.503393e-01   \n",
      "max   2.868475e+00  1.064581e+00              3.101718e+00  1.176001e+00   \n",
      "\n",
      "      ejection_fraction  high_blood_pressure     platelets  serum_creatinine  \\\n",
      "mean      -1.804112e-16         4.758099e-17 -4.807662e-16      7.930164e-17   \n",
      "min       -2.069263e+00        -7.819916e-01 -1.997878e+00     -1.616121e+00   \n",
      "max        2.295445e+00         1.278786e+00  2.554110e+00      2.970169e+00   \n",
      "\n",
      "      serum_sodium           sex       smoking          time   DEATH_EVENT  \n",
      "mean -1.056694e-15 -8.326673e-17 -1.586033e-17 -1.903239e-16  6.344132e-17  \n",
      "min  -3.171117e+00 -1.341641e+00 -6.882472e-01 -1.683733e+00 -6.117460e-01  \n",
      "max   2.884329e+00  7.453560e-01  1.452966e+00  1.993861e+00  1.634665e+00  \n",
      "\n",
      "Capping:\n",
      "Coluna: age | P5: -1.58 | P95: 1.75\n",
      "Coluna: anaemia | P5: -0.94 | P95: 1.06\n",
      "Coluna: creatinine_phosphokinase | P5: -1.00 | P95: 2.01\n",
      "Coluna: diabetes | P5: -0.85 | P95: 1.18\n",
      "Coluna: ejection_fraction | P5: -1.56 | P95: 1.87\n",
      "Coluna: high_blood_pressure | P5: -0.78 | P95: 1.28\n",
      "Coluna: platelets | P5: -1.59 | P95: 1.87\n",
      "Coluna: serum_creatinine | P5: -1.31 | P95: 2.14\n",
      "Coluna: serum_sodium | P5: -1.85 | P95: 1.83\n",
      "Coluna: sex | P5: -1.34 | P95: 0.75\n",
      "Coluna: smoking | P5: -0.69 | P95: 1.45\n",
      "Coluna: time | P5: -1.55 | P95: 1.48\n",
      "Coluna: DEATH_EVENT | P5: -0.61 | P95: 1.63\n",
      "\n",
      "Impacto do capping:\n",
      "           age_raw   anaemia_raw  creatinine_phosphokinase_raw  diabetes_raw  \\\n",
      "mean  2.220446e-16  5.749369e-17                  4.758099e-17 -6.344132e-17   \n",
      "std   1.002240e+00  1.002240e+00                  1.002240e+00  1.002240e+00   \n",
      "\n",
      "      ejection_fraction_raw  high_blood_pressure_raw  platelets_raw  \\\n",
      "mean          -1.804112e-16             4.758099e-17  -4.807662e-16   \n",
      "std            1.002240e+00             1.002240e+00   1.002240e+00   \n",
      "\n",
      "      serum_creatinine_raw  serum_sodium_raw       sex_raw  ...      diabetes  \\\n",
      "mean          7.930164e-17     -1.056694e-15 -8.326673e-17  ... -6.344132e-17   \n",
      "std           1.002240e+00      1.002240e+00  1.002240e+00  ...  1.002240e+00   \n",
      "\n",
      "      ejection_fraction  high_blood_pressure  platelets  serum_creatinine  \\\n",
      "mean           0.004585         4.758099e-17  -0.003331         -0.010783   \n",
      "std            0.983701         1.002240e+00   0.947854          0.958247   \n",
      "\n",
      "      serum_sodium           sex       smoking      time   DEATH_EVENT  \n",
      "mean      0.004701 -8.326673e-17 -1.586033e-17 -0.006076  6.344132e-17  \n",
      "std       0.926087  1.002240e+00  1.002240e+00  0.982388  1.002240e+00  \n",
      "\n",
      "[2 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "#Explorar distribuição inicial\n",
    "print(\"\\nDistribuição inicial:\")\n",
    "num_cols =df_heart_cleaned.select_dtypes(include=[np.number]).columns\n",
    "print(df_heart_cleaned[num_cols].describe().loc[['mean', 'min', 'max']])\n",
    "\n",
    "#Calcular p5/p95 e aplicar capping\n",
    "print(\"\\nCapping:\")\n",
    "for col in num_cols:\n",
    "    p5 = df_heart_cleaned[col].quantile(0.05)\n",
    "    p95 = df_heart_cleaned[col].quantile(0.95)\n",
    "    df_heart_cleaned[col + \"_raw\"] = df_heart_cleaned[col] #guardar os valores originais\n",
    "    print(f\"Coluna: {col} | P5: {p5:.2f} | P95: {p95:.2f}\")\n",
    "    df_heart_cleaned[col] = df_heart_cleaned[col].clip(p5, p95)   \n",
    "    #print(f\"Coluna: {col} | Média: {df_heart_cleaned[col].mean():.2f} | Mínimo: {df_heart_cleaned[col].min():.2f} | Máximo: {df_heart_cleaned[col].max():.2f}\")\n",
    "\n",
    "#Avaliar impacto do capping\n",
    "before = df_heart_cleaned[[col + \"_raw\" for col in num_cols]].describe().loc[['mean', 'std']]\n",
    "after = df_heart_cleaned[num_cols].describe().loc[['mean', 'std']]\n",
    "print(\"\\nImpacto do capping:\")\n",
    "print(before.join(after, lsuffix='_before', rsuffix='_after'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2b8cfb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de duplicados: 0\n",
      "\n",
      "Valores ausentes finais:\n",
      "age                             0\n",
      "anaemia                         0\n",
      "creatinine_phosphokinase        0\n",
      "diabetes                        0\n",
      "ejection_fraction               0\n",
      "high_blood_pressure             0\n",
      "platelets                       0\n",
      "serum_creatinine                0\n",
      "serum_sodium                    0\n",
      "sex                             0\n",
      "smoking                         0\n",
      "time                            0\n",
      "DEATH_EVENT                     0\n",
      "age_raw                         0\n",
      "anaemia_raw                     0\n",
      "creatinine_phosphokinase_raw    0\n",
      "diabetes_raw                    0\n",
      "ejection_fraction_raw           0\n",
      "high_blood_pressure_raw         0\n",
      "platelets_raw                   0\n",
      "serum_creatinine_raw            0\n",
      "serum_sodium_raw                0\n",
      "sex_raw                         0\n",
      "smoking_raw                     0\n",
      "time_raw                        0\n",
      "DEATH_EVENT_raw                 0\n",
      "dtype: int64\n",
      "\n",
      "Colunas numéricas escalonadas:\n",
      "        age   anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
      "0  1.190537 -0.939336                  0.878670 -0.850339          -1.555768   \n",
      "2  0.351568 -0.939336                 -0.684634 -0.850339          -1.555768   \n",
      "3 -0.906886  1.064581                 -0.810128 -0.850339          -1.555768   \n",
      "5  1.752646  1.064581                 -0.996040 -0.850339           0.155882   \n",
      "6  1.190537  1.064581                 -0.326078 -0.850339          -1.555768   \n",
      "\n",
      "   high_blood_pressure  platelets  serum_creatinine  serum_sodium       sex  \\\n",
      "0             1.278786   0.136333          2.144637     -1.854716  0.745356   \n",
      "2            -0.781992  -1.400896          0.524147     -1.854716  0.745356   \n",
      "3            -0.781992  -0.684517          2.144637     -0.011754  0.745356   \n",
      "5             1.278786  -0.774065          2.144637     -1.328155  0.745356   \n",
      "6            -0.781992  -1.594915          0.218395     -0.011754  0.745356   \n",
      "\n",
      "    smoking      time  DEATH_EVENT  \n",
      "0 -0.688247 -1.550895     1.634665  \n",
      "2  1.452966 -1.550895     1.634665  \n",
      "3 -0.688247 -1.550895     1.634665  \n",
      "5  1.452966 -1.550895     1.634665  \n",
      "6 -0.688247 -1.550895     1.634665  \n",
      "\n",
      "Colunas categóricas codificadas:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 2, 3, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "#G. Validação final\n",
    "\n",
    "# Verificar duplicados\n",
    "duplicates = df_heart_cleaned.duplicated().sum()\n",
    "print(f'Número de duplicados: {duplicates}')\n",
    "\n",
    "# Verificar ausência de valores nulos\n",
    "print(f'\\nValores ausentes finais:\\n{df_heart_cleaned.isnull().sum()}')\n",
    "\n",
    "# Verificar colunas numéricas escalonadas\n",
    "print(f'\\nColunas numéricas escalonadas:\\n{df_heart_cleaned[numeric_columns].head()}')\n",
    "\n",
    "# Verificar colunas categóricas codificadas (One-Hot ou Label Encoding)\n",
    "print(f'\\nColunas categóricas codificadas:\\n{df_heart_cleaned.select_dtypes(include=[object]).head()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b499e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H. Exportação\n",
    "\n",
    "# Exportar para Pandas\n",
    "DATA_PATH_PANDAS = r'C:\\Users\\HP\\Desktop\\Formação\\Eisnt\\UFCD 10808 - Limpeza e transformação de dados em Python\\Avaliacao\\heart_prepared_pandas.csv'\n",
    "\n",
    "df_heart_cleaned.to_csv(DATA_PATH_PANDAS, index=False)\n",
    "\n",
    "# Exportar para Polars\n",
    "import polars as pl\n",
    "\n",
    "DATA_PATH_POLARS = r'C:\\Users\\HP\\Desktop\\Formação\\Eisnt\\UFCD 10808 - Limpeza e transformação de dados em Python\\Avaliacao\\heart_prepared_polars.csv'\n",
    "\n",
    "df_polars = pl.from_pandas(df_heart_cleaned)\n",
    "df_polars.write_csv(DATA_PATH_POLARS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
