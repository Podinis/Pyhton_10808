{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6135437",
   "metadata": {},
   "source": [
    "#### Carregamento de bibliotecas e DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "296b1c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        age       sex       bmi        bp        s1        s2        s3  \\\n",
      "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
      "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
      "2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
      "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
      "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
      "\n",
      "         s4        s5        s6  target  \n",
      "0 -0.002592  0.019907 -0.017646   151.0  \n",
      "1 -0.039493 -0.068332 -0.092204    75.0  \n",
      "2 -0.002592  0.002861 -0.025930   141.0  \n",
      "3  0.034309  0.022688 -0.009362   206.0  \n",
      "4 -0.002592 -0.031988 -0.046641   135.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "#Carrega o dataset\n",
    "df = pd.read_csv(r'C:\\Users\\HP\\Desktop\\Formação\\Eisnt\\UFCD 10808 - Limpeza e transformação de dados em Python\\Avaliacao\\diabetes_dataset.csv', sep=',')\n",
    "df_polars = pl.read_csv(r'C:\\Users\\HP\\Desktop\\Formação\\Eisnt\\UFCD 10808 - Limpeza e transformação de dados em Python\\Avaliacao\\diabetes_dataset.csv', separator=',')  \n",
    "\n",
    "# Verifica se o dataframe foi carregado corretamente\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8e6740",
   "metadata": {},
   "source": [
    "#### Exercício 1 – Deteção de Outliers Pandas (IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efcae860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercicio 1 - Número de outliers detectados: {'bmi': 3, 'bp': 0, 's5': 4}\n"
     ]
    }
   ],
   "source": [
    "def detectar_outliers_iqr(df, cols):\n",
    "    outliers_info = {}\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            limite_inferior = Q1 - 1.5 * IQR\n",
    "            limite_superior = Q3 + 1.5 * IQR\n",
    "            outliers = df[(df[col] < limite_inferior) | (df[col] > limite_superior)]\n",
    "            outliers_info[col] = len(outliers)\n",
    "            \n",
    "    return outliers_info\n",
    "\n",
    "outliers_ex1 = detectar_outliers_iqr(df, ['bmi', 'bp', 's5'])\n",
    "\n",
    "print('Exercicio 1 - Número de outliers detectados:', outliers_ex1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e35d98b",
   "metadata": {},
   "source": [
    "#### Exercício 2 – Capping de Valores Extremos Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12d140f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercicio 2 - Capping aplicado\n"
     ]
    }
   ],
   "source": [
    "def aplicar_capping(df, cols):\n",
    "    capped_df = df.copy()\n",
    "    for col in cols:\n",
    "        p5 = df[col].quantile(0.05)\n",
    "        p95 = df[col].quantile(0.95)\n",
    "        capped_df[col] = capped_df[col].clip(lower=p5, upper=p95)\n",
    "    return capped_df\n",
    "\n",
    "df_capped = aplicar_capping(df, ['bmi', 'bp', 's5'])\n",
    "\n",
    "print('Exercicio 2 - Capping aplicado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3e15f8",
   "metadata": {},
   "source": [
    "#### Exercício 3 – Duplicados e Valores Nulos (simulados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a5b4b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercício 3 - Duplicados antes: 1\n",
      "Exercício 3 - Duplicados depois: 0\n",
      "\n",
      "Exercicio 3 - Nulos e duplicados removidos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_29452\\503058056.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df3['bmi'].fillna(df3['bmi'].median(), inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_29452\\503058056.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df3['bp'].fillna(df3['bp'].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df3 = df.copy()\n",
    "df3.loc[0, 'bmi'] = None\n",
    "df3.loc[1, 'bp'] = None\n",
    "df3 = pd.concat([df3, df3.iloc[[2]]]) \n",
    "\n",
    "duplicados_ex3 = df3.duplicated().sum()\n",
    "print('Exercício 3 - Duplicados antes:', duplicados_ex3)\n",
    "\n",
    "##remove duplicados\n",
    "df3 = df3.drop_duplicates()\n",
    "df3['bmi'].fillna(df3['bmi'].median(), inplace=True)\n",
    "df3['bp'].fillna(df3['bp'].median(), inplace=True)\n",
    "\n",
    "df_depois = df3.duplicated().sum()\n",
    "print('Exercício 3 - Duplicados depois:', df_depois)\n",
    "\n",
    "print('\\nExercicio 3 - Nulos e duplicados removidos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125b89e5",
   "metadata": {},
   "source": [
    "#### Exercício 4 – Encoding de Variáveis Categóricas (simuladas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d168af05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercicio 4 - Codificação categórica concluída\n"
     ]
    }
   ],
   "source": [
    "df4 = df.copy()\n",
    "df4['categoria_idade'] = pd.cut(df4['age'], bins=[0, 0.3, 0.6, 1.0], labels=['jovem', 'adulto', 'sénior'])\n",
    "df4_label_encoded = df4.copy()\n",
    "df4_label_encoded['categoria_idade'] = df4_label_encoded['categoria_idade'].astype('category').cat.codes\n",
    "df4_onehot_encoded = pd.get_dummies(df4, columns=['categoria_idade'], drop_first=True)\n",
    "print('Exercicio 4 - Codificação categórica concluída')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f8ac84",
   "metadata": {},
   "source": [
    "#### Exercício 5 – Escalonamento de Variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5adcf2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercicio 5 - Escalonamento aplicado\n"
     ]
    }
   ],
   "source": [
    "df5 = df.copy()\n",
    "scaler = StandardScaler()\n",
    "df5_scaled = df5.copy()\n",
    "df5_scaled[df5.columns] = scaler.fit_transform(df5[df5.columns])\n",
    "\n",
    "print('Exercicio 5 - Escalonamento aplicado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd85234d",
   "metadata": {},
   "source": [
    "#### Exercício 6 – Feature Engineering + Seleção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be6248ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercício 6 - Features selecionadas: ['age', 'bmi', 'bp', 's4', 's5']\n"
     ]
    }
   ],
   "source": [
    "df6 = df.copy()\n",
    "df6['bmi_bp_interaction'] = df6['bmi'] * df6['bp']\n",
    "#print(df6.head())\n",
    "\n",
    "X = df6.drop(columns=['s6']) \n",
    "y = (df6['s6'] > df6['s6'].median()).astype(int)\n",
    "\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "selector = SelectKBest(score_func=f_classif, k=5)\n",
    "X_selected = selector.fit_transform(X_scaled, y)\n",
    "selected_features = X.columns[selector.get_support()].tolist()\n",
    "\n",
    "print('Exercício 6 - Features selecionadas:', selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba29c4",
   "metadata": {},
   "source": [
    "## Polars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef086f58",
   "metadata": {},
   "source": [
    "#### Exercício 1 – Deteção de Outliers Polars (IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6efbf7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercicio 1 - Outliers encontrados: {'bmi': 3, 'bp': 0, 's5': 4}\n"
     ]
    }
   ],
   "source": [
    "def detectar_outliers_iqr(df, cols):\n",
    "    outliers_info = {}\n",
    "    for col in cols:\n",
    "        q1 = df.select(pl.col(col).quantile(0.25)).item()\n",
    "        q3 = df.select(pl.col(col).quantile(0.75)).item()\n",
    "        iqr = q3 - q1\n",
    "        limite_inferior = q1 - 1.5 * iqr\n",
    "        limite_superior = q3 + 1.5 * iqr\n",
    "        outliers = df.filter((pl.col(col) < limite_inferior) | (pl.col(col) > limite_superior))\n",
    "        outliers_info[col] = outliers.height\n",
    "    return outliers_info\n",
    "\n",
    "outliers_ex1 = detectar_outliers_iqr(df_polars, ['bmi', 'bp', 's5'])\n",
    "print('Exercicio 1 - Outliers encontrados:', outliers_ex1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248c871b",
   "metadata": {},
   "source": [
    "#### Exercício 2 – Capping de Valores Extremos Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec6424c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercício 2 - Capping aplicado\n"
     ]
    }
   ],
   "source": [
    "def aplicar_capping(df, cols):\n",
    "    df_capped = df.clone()\n",
    "    for col in cols:\n",
    "        p5 = df.select(pl.col(col).quantile(0.05)).item()\n",
    "        p95 = df.select(pl.col(col).quantile(0.95)).item()\n",
    "        df_capped = df_capped.with_columns(\n",
    "            pl.col(col).clip(lower_bound=p5, upper_bound=p95).alias(col)\n",
    "        )\n",
    "    return df_capped\n",
    "\n",
    "df_capped = aplicar_capping(df_polars, ['bmi', 'bp', 's5'])\n",
    "print('Exercício 2 - Capping aplicado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b2dae",
   "metadata": {},
   "source": [
    "#### Exercício 3 – Duplicados e Valores Nulos (simulados) Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e812c40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercício 3 - Duplicados antes: 2\n",
      "Exercício 3 - Duplicados depois: 0\n",
      "\n",
      "Exercício 3 - Nulos imputados e duplicados removidos\n"
     ]
    }
   ],
   "source": [
    "df3 = df_polars.clone()\n",
    "df3 = df3.with_columns([\n",
    "    pl.when(pl.arange(0, df3.height) == 0).then(None).otherwise(pl.col('bmi')).alias('bmi'),\n",
    "    pl.when(pl.arange(0, df3.height) == 1).then(None).otherwise(pl.col('bp')).alias('bp')\n",
    "])\n",
    "\n",
    "df3 = df3.vstack(df3[2:3]) \n",
    "\n",
    "duplicados_ex3 = df3.is_duplicated().sum()\n",
    "print('Exercício 3 - Duplicados antes:', duplicados_ex3)\n",
    "\n",
    "# remove duplicados\n",
    "df3 = df3.unique()\n",
    "df3 = df3.with_columns([\n",
    "    pl.col('bmi').fill_null(pl.col('bmi').median()),\n",
    "    pl.col('bp').fill_null(pl.col('bp').median())\n",
    "])\n",
    "duplicados_depois = df3.is_duplicated().sum()\n",
    "print('Exercício 3 - Duplicados depois:', duplicados_depois)\n",
    "\n",
    "print('\\nExercício 3 - Nulos imputados e duplicados removidos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80de4762",
   "metadata": {},
   "source": [
    "#### Exercício 4 – Encoding de Variáveis Categóricas (simuladas) Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a288f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercício 4 - Codificação categórica concluída\n"
     ]
    }
   ],
   "source": [
    "df4 = df_polars.clone()\n",
    "df4 = df4.with_columns([\n",
    "    pl.when(pl.col('age') <= 0.3).then(pl.lit('jovem'))\n",
    "    .when(pl.col('age') <= 0.6).then(pl.lit('adulto'))\n",
    "    .otherwise(pl.lit('sénior'))\n",
    "    .alias('categoria_idade')\n",
    "])\n",
    "\n",
    "# Label Encoding\n",
    "df4_label_encoded = df4.with_columns([\n",
    "    pl.col('categoria_idade').cast(pl.Categorical)\n",
    "])\n",
    "\n",
    "# One-Hot Encoding\n",
    "df4_label_encoded = df4.to_dummies(columns=['categoria_idade'])\n",
    "print('Exercício 4 - Codificação categórica concluída')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b5940d",
   "metadata": {},
   "source": [
    "#### Exercício 5 – Escalonamento de Variáveis Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55f25311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 11)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ age       ┆ sex       ┆ bmi       ┆ bp        ┆ … ┆ s4        ┆ s5        ┆ s6        ┆ target   │\n",
      "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
      "│ f64       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 0.8005    ┆ 1.065488  ┆ 1.297088  ┆ 0.459841  ┆ … ┆ -0.054499 ┆ 0.418531  ┆ -0.370989 ┆ -0.01471 │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 9        │\n",
      "│ -0.039567 ┆ -0.938537 ┆ -1.08218  ┆ -0.553505 ┆ … ┆ -0.830301 ┆ -1.436589 ┆ -1.938479 ┆ -1.00165 │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 9        │\n",
      "│ 1.793307  ┆ 1.065488  ┆ 0.934533  ┆ -0.119214 ┆ … ┆ -0.054499 ┆ 0.060156  ┆ -0.545154 ┆ -0.14458 │\n",
      "│ -1.872441 ┆ -0.938537 ┆ -0.243771 ┆ -0.77065  ┆ … ┆ 0.721302  ┆ 0.476983  ┆ -0.196823 ┆ 0.699513 │\n",
      "│ 0.113172  ┆ -0.938537 ┆ -0.764944 ┆ 0.459841  ┆ … ┆ -0.054499 ┆ -0.672502 ┆ -0.980568 ┆ -0.22249 │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 6        │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n",
      "Exercício 5 - Escalonamento aplicado\n"
     ]
    }
   ],
   "source": [
    "df5 = df_polars.clone()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "df5_scaled_np = scaler.fit_transform(df5.to_numpy())\n",
    "df5_scaled = pl.DataFrame(df5_scaled_np, schema=df_polars.columns)\n",
    "print(df5_scaled.head())\n",
    "print('Exercício 5 - Escalonamento aplicado')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8275b55",
   "metadata": {},
   "source": [
    "#### Exercício 6 – Feature Engineering + Seleção (Polars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a541a065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercício 6 - Features selecionadas: ['age', 'bmi', 'bp', 's4', 's5']\n"
     ]
    }
   ],
   "source": [
    "df6 = df_polars.clone()\n",
    "df6 = df6.with_columns([\n",
    "    (pl.col('bmi') * pl.col('bp')).alias('bmi_bp_interaction')\n",
    "])\n",
    "\n",
    "X = df6.drop('s6').to_pandas()\n",
    "y = (df6['s6'].to_numpy() > df6['s6'].median()).astype(int)\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "selector = SelectKBest(score_func=f_classif, k=5)\n",
    "X_selected = selector.fit_transform(X_scaled, y)\n",
    "selected_features = X.columns[selector.get_support()].tolist()\n",
    "\n",
    "print('Exercício 6 - Features selecionadas:', selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9473d380",
   "metadata": {},
   "source": [
    "### Exercício 7 em PANDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e45d0f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicados antes: 0\n",
      "Duplicados depois: 0\n",
      "\n",
      "Dataset final pronto para modelos.\n",
      "     Row ID  Postal Code     Sales  Ship Mode_Same Day  \\\n",
      "0 -1.731874    -0.401493  0.049776               False   \n",
      "1 -1.731521    -0.401493  0.799801               False   \n",
      "2 -1.731167     1.085497 -0.344944               False   \n",
      "3 -1.730814    -0.685956  1.159887               False   \n",
      "4 -1.730460    -0.685956 -0.332580               False   \n",
      "\n",
      "   Ship Mode_Second Class  Ship Mode_Standard Class  Segment_Corporate  \\\n",
      "0                    True                     False              False   \n",
      "1                    True                     False              False   \n",
      "2                    True                     False               True   \n",
      "3                   False                      True              False   \n",
      "4                   False                      True              False   \n",
      "\n",
      "   Segment_Home Office  City_Abilene  City_Akron  ...  Sub-Category_Envelopes  \\\n",
      "0                False         False       False  ...                   False   \n",
      "1                False         False       False  ...                   False   \n",
      "2                False         False       False  ...                   False   \n",
      "3                False         False       False  ...                   False   \n",
      "4                False         False       False  ...                   False   \n",
      "\n",
      "   Sub-Category_Fasteners  Sub-Category_Furnishings  Sub-Category_Labels  \\\n",
      "0                   False                     False                False   \n",
      "1                   False                     False                False   \n",
      "2                   False                     False                 True   \n",
      "3                   False                     False                False   \n",
      "4                   False                     False                False   \n",
      "\n",
      "   Sub-Category_Machines  Sub-Category_Paper  Sub-Category_Phones  \\\n",
      "0                  False               False                False   \n",
      "1                  False               False                False   \n",
      "2                  False               False                False   \n",
      "3                  False               False                False   \n",
      "4                  False               False                False   \n",
      "\n",
      "   Sub-Category_Storage  Sub-Category_Supplies  Sub-Category_Tables  \n",
      "0                 False                  False                False  \n",
      "1                 False                  False                False  \n",
      "2                 False                  False                False  \n",
      "3                 False                  False                 True  \n",
      "4                  True                  False                False  \n",
      "\n",
      "[5 rows x 605 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_29452\\743932293.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Postal Code'].fillna(df['Postal Code'].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1 Carrega o dataset\n",
    "\n",
    "DATA_PATH = r'C:\\Users\\HP\\Desktop\\Formação\\Eisnt\\UFCD 10808 - Limpeza e transformação de dados em Python\\Avaliacao\\train.csv'\n",
    "df = pd.read_csv(DATA_PATH, sep=',')\n",
    "#print(df.head())\n",
    "\n",
    "# 2. Tratamento de valores nulos e duplicados\n",
    "print('Duplicados antes:', df.duplicated().sum())\n",
    "df = df.drop_duplicates()\n",
    "print('Duplicados depois:', df.duplicated().sum())\n",
    "\n",
    "df['Postal Code'].fillna(df['Postal Code'].median(), inplace=True)\n",
    "\n",
    "# 3. Codificação de variáveis categóricas\n",
    "cat_cols = df.select_dtypes(include='object').columns\n",
    "\n",
    "drop_cols = ['Order ID', 'Order Date', 'Ship Date', 'Customer ID', 'Customer Name',\n",
    "'Product ID', 'Product Name']\n",
    "df = df.drop(columns=drop_cols)\n",
    "\n",
    "df_encoded = pd.get_dummies(df, columns=[col for col in cat_cols if col not in drop_cols], drop_first=True)\n",
    "\n",
    "# 4. Escalonamento de variáveis numéricas\n",
    "scaler = StandardScaler()\n",
    "num_cols = df_encoded.select_dtypes(include='number').columns\n",
    "df_encoded[num_cols] = scaler.fit_transform(df_encoded[num_cols])\n",
    "\n",
    "# 5. Dataset pronto para modelos\n",
    "print('\\nDataset final pronto para modelos.')\n",
    "print(df_encoded.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026b4ddf",
   "metadata": {},
   "source": [
    "### Exercício 7 em POLARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ad2526d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicados antes: 0\n",
      "Duplicados depois: 0\n",
      "\n",
      " Dataset final pronto para modelos.\n",
      "shape: (5, 613)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ Row ID    ┆ Ship Mode ┆ Ship      ┆ Ship Mode ┆ … ┆ Sub-Categ ┆ Sub-Categ ┆ Sub-Categ ┆ Sales    │\n",
      "│ ---       ┆ _First    ┆ Mode_Same ┆ _Second   ┆   ┆ ory_Stora ┆ ory_Suppl ┆ ory_Table ┆ ---      │\n",
      "│ f64       ┆ Class     ┆ Day       ┆ Class     ┆   ┆ ge        ┆ ies       ┆ s         ┆ f64      │\n",
      "│           ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆          │\n",
      "│           ┆ u8        ┆ u8        ┆ u8        ┆   ┆ u8        ┆ u8        ┆ u8        ┆          │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 0.101625  ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 0         ┆ 0         ┆ 0         ┆ -0.04916 │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 7        │\n",
      "│ 0.874332  ┆ 0         ┆ 0         ┆ 1         ┆ … ┆ 0         ┆ 1         ┆ 0         ┆ -0.19255 │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 5        │\n",
      "│ 0.997697  ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 0         ┆ 0         ┆ 0         ┆ -0.30326 │\n",
      "│ -0.282607 ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 0         ┆ 0         ┆ 0         ┆ -0.36466 │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 3        │\n",
      "│ 0.027395  ┆ 1         ┆ 0         ┆ 0         ┆ … ┆ 0         ┆ 0         ┆ 0         ┆ -0.35760 │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 3        │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Carregamento do dataset\n",
    "DATA_PATH = r'C:\\Users\\HP\\Desktop\\Formação\\Eisnt\\UFCD 10808 - Limpeza e transformação de dados em Python\\Avaliacao\\train.csv'\n",
    "df = pl.read_csv(DATA_PATH, separator=',')\n",
    "\n",
    "# 2. Tratamento de valores nulos e duplicados\n",
    "duplicados_antes = df.is_duplicated().sum()\n",
    "\n",
    "print('Duplicados antes:', duplicados_antes)\n",
    "df = df.unique()\n",
    "print('Duplicados depois:', df.is_duplicated().sum())\n",
    "\n",
    "postal_median = df.select(pl.col('Postal Code').median()).item()\n",
    "df = df.with_columns(\n",
    "            pl.col('Postal Code').fill_null(postal_median)\n",
    "            )\n",
    "\n",
    "# 3. Codificação de variáveis categóricas\n",
    "drop_cols = ['Order ID', 'Order Date', 'Ship Date', 'Customer ID', 'Customer Name','Product ID', 'Product Name']\n",
    "\n",
    "df = df.drop(drop_cols)\n",
    "\n",
    "cat_cols = [col for col, dtype in zip(df.columns, df.dtypes) if dtype == pl.Utf8]\n",
    "\n",
    "df = df.to_dummies(columns=cat_cols)\n",
    "\n",
    "# 4. Escalonamento de variáveis numéricas\n",
    "scaler = StandardScaler()\n",
    "numeric_cols = [col for col, dtype in zip(df.columns, df.dtypes) if dtype in (pl.Int64, pl.Float64)]\n",
    "df_scaled_np = scaler.fit_transform(df[numeric_cols].to_numpy())\n",
    "df_scaled = df.with_columns([\n",
    "pl.Series(name=col, values=df_scaled_np[:, i]) for i, col in enumerate(numeric_cols)\n",
    "])\n",
    "\n",
    "# 5. Dataset pronto para modelos\n",
    "print('\\n Dataset final pronto para modelos.')\n",
    "print(df_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbf7d57",
   "metadata": {},
   "source": [
    "## Desafio Autónomo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b476eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicados antes: 0\n",
      "Duplicados depois: 0\n",
      "\n",
      " Dimensões do dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 299 entries, 0 to 298\n",
      "Data columns (total 13 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   age                       299 non-null    float64\n",
      " 1   anaemia                   299 non-null    int64  \n",
      " 2   creatinine_phosphokinase  299 non-null    int64  \n",
      " 3   diabetes                  299 non-null    int64  \n",
      " 4   ejection_fraction         299 non-null    int64  \n",
      " 5   high_blood_pressure       299 non-null    int64  \n",
      " 6   platelets                 299 non-null    float64\n",
      " 7   serum_creatinine          299 non-null    float64\n",
      " 8   serum_sodium              299 non-null    int64  \n",
      " 9   sex                       299 non-null    int64  \n",
      " 10  smoking                   299 non-null    int64  \n",
      " 11  time                      299 non-null    int64  \n",
      " 12  DEATH_EVENT               299 non-null    int64  \n",
      "dtypes: float64(3), int64(10)\n",
      "memory usage: 30.5 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# 1. Carregamento do dataset\n",
    "DATA_PATH = r'C:\\Users\\HP\\Desktop\\Formação\\Eisnt\\UFCD 10808 - Limpeza e transformação de dados em Python\\Pyhton_10808\\Avaliacao\\heart_failure_clinical_records_dataset.csv'\n",
    "df_heart = pd.read_csv(DATA_PATH, sep=',', na_values=[\"N/D\", \"NA\"])\n",
    "#print(df_heart.head())\n",
    "\n",
    "# A. Exploração inicial (EDA breve)\n",
    "#Tratamento de valores nulos e duplicados, dimensoes do dataset\n",
    "print('Duplicados antes:', df_heart.duplicated().sum())\n",
    "df_heart = df_heart.drop_duplicates()\n",
    "print('Duplicados depois:', df_heart.duplicated().sum())\n",
    "\n",
    "print(\"\\n Dimensões do dataset:\")\n",
    "print(df_heart.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d01210e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers encontrados: {'age': 0, 'creatinine_phosphokinase': 29, 'ejection_fraction': 2, 'platelets': 21, 'serum_creatinine': 29, 'serum_sodium': 4, 'time': 0}\n"
     ]
    }
   ],
   "source": [
    "# B. Limpeza de outliers\n",
    "def detectar_outliers_iqr(df, cols):\n",
    "    outliers_info = {}\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            limite_inferior = Q1 - 1.5 * IQR\n",
    "            limite_superior = Q3 + 1.5 * IQR\n",
    "            outliers = df[(df[col] < limite_inferior) | (df[col] > limite_superior)]\n",
    "            outliers_info[col] = len(outliers)\n",
    "            \n",
    "    return outliers, outliers_info\n",
    "\n",
    "colunas_para_verificar = ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']\n",
    "\n",
    "df_heart_cleaned,outliers_info = detectar_outliers_iqr(df_heart, colunas_para_verificar)\n",
    "print('Outliers encontrados:', outliers_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3049069f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contagem de nulos\n",
      "age                         0\n",
      "anaemia                     0\n",
      "creatinine_phosphokinase    0\n",
      "diabetes                    0\n",
      "ejection_fraction           0\n",
      "high_blood_pressure         0\n",
      "platelets                   0\n",
      "serum_creatinine            0\n",
      "serum_sodium                0\n",
      "sex                         0\n",
      "smoking                     0\n",
      "time                        0\n",
      "DEATH_EVENT                 0\n",
      "categoria_idade             0\n",
      "categoria_sexo              0\n",
      "categoria_serum             0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_29452\\3911506035.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_heart_cleaned[col].fillna(df_heart_cleaned[col].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# C. Tratamento de valores em falta\n",
    "    \n",
    "print('\\nContagem de nulos')\n",
    "print(df_heart.isnull().sum()) #verifica se existem valores nulos\n",
    "df_heart = df_heart.dropna() #remove os valores nulos\n",
    "\n",
    "# Imputação de valores em falta\n",
    "# Preencher variáveis numéricas, usamos a mediana\n",
    "for col in df_heart_cleaned.select_dtypes(include=[np.number]).columns:\n",
    "    df_heart_cleaned[col].fillna(df_heart_cleaned[col].median(), inplace=True)\n",
    "\n",
    "#Imputação de valores em falta para variáveis categóricas com moda\n",
    "cat_cols = df_heart_cleaned.select_dtypes(include=[object]).columns\n",
    "\n",
    "for col in cat_cols:\n",
    "    moda = df_heart_cleaned[col].mode()[0]\n",
    "    df_heart_cleaned[col].fillna(moda, inplace=True)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9f93186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribuição de idades:\n",
      "categoria_idade\n",
      "60-79         145\n",
      "40-59         129\n",
      "80 ou mais     25\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribuição de Serum:\n",
      "categoria_serum\n",
      "Bom        205\n",
      "Baixo       83\n",
      "Elevado     11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribuição de Sexo:\n",
      "categoria_sexo\n",
      "M    194\n",
      "F    105\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#D. Engenharia de features\n",
    "def categorizar_idade(idade):\n",
    "    if idade < 40:\n",
    "        return 'Menos de 40'\n",
    "    elif 40 <= idade < 60:\n",
    "        return '40-59'\n",
    "    elif 60 <= idade < 80:\n",
    "        return '60-79'\n",
    "    else:\n",
    "        return '80 ou mais'\n",
    "    \n",
    "#recurso a função\n",
    "df_heart['categoria_idade'] = df_heart['age'].apply(categorizar_idade)\n",
    "# recurso a numpy where\n",
    "df_heart['categoria_sexo'] = np.where(df_heart['sex'] > 0, 'M', 'F')\n",
    "# recurso a lambda\n",
    "df_heart['categoria_serum'] = df_heart['serum_sodium'].apply(lambda x: 'Baixo' if x < 135 else 'Bom' if x < 145 else 'Elevado' )\n",
    "\n",
    "print ('\\nDistribuição de idades:')\n",
    "print(df_heart['categoria_idade'].value_counts())\n",
    "print ('\\nDistribuição de Serum:')\n",
    "print(df_heart['categoria_serum'].value_counts())\n",
    "print ('\\nDistribuição de Sexo:')\n",
    "print(df_heart['categoria_sexo'].value_counts())\n",
    "#print(df_heart.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ee91f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoding: Empty DataFrame\n",
      "Columns: [age, anaemia, creatinine_phosphokinase, diabetes, ejection_fraction, high_blood_pressure, serum_creatinine, serum_sodium, sex, smoking, time, DEATH_EVENT]\n",
      "Index: []\n",
      "\n",
      "Label Encoding: Empty DataFrame\n",
      "Columns: [age, anaemia, creatinine_phosphokinase, diabetes, ejection_fraction, high_blood_pressure, serum_creatinine, serum_sodium, sex, smoking, time, DEATH_EVENT]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#E. Codificação de variáveis categóricas\n",
    "\n",
    "# Aplicar One-Hot Encoding \n",
    "df_one_hot = pd.get_dummies(df_heart_cleaned, drop_first=True)\n",
    "\n",
    "# Label Encoding para variáveis categóricas\n",
    "label_encoder = LabelEncoder()  #Inicializando o LabelEncoder\n",
    "cat_cols = df_heart_cleaned.select_dtypes(include=[object]).columns\n",
    "df_label_encoded = df_heart_cleaned.copy()\n",
    "\n",
    "for col in cat_cols:\n",
    "    df_label_encoded[col] = label_encoder.fit_transform(df_label_encoded[col])\n",
    "\n",
    "# Verificar resultados\n",
    "print(\"One-Hot Encoding:\", df_one_hot.head())\n",
    "print(\"\\nLabel Encoding:\", df_label_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3e32bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipos de dados antes da normalização:\n",
      "age                         float64\n",
      "anaemia                       int64\n",
      "creatinine_phosphokinase      int64\n",
      "diabetes                      int64\n",
      "ejection_fraction             int64\n",
      "high_blood_pressure           int64\n",
      "serum_creatinine            float64\n",
      "serum_sodium                  int64\n",
      "sex                           int64\n",
      "smoking                       int64\n",
      "time                          int64\n",
      "DEATH_EVENT                   int64\n",
      "dtype: object\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 12)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Normalizar todas as variáveis numéricas (int e float)\u001b[39;00m\n\u001b[32m     10\u001b[39m numeric_columns = df_heart_cleaned.select_dtypes(include=\u001b[33m\"\u001b[39m\u001b[33mnumber\u001b[39m\u001b[33m\"\u001b[39m).columns\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m df_heart_cleaned[numeric_columns] = \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_heart_cleaned\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnumeric_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Conversão dos valores normalizados para int\u001b[39;00m\n\u001b[32m     14\u001b[39m df_heart_cleaned[numeric_columns] = df_heart_cleaned[numeric_columns].round().astype(\u001b[33m'\u001b[39m\u001b[33mint\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:918\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    903\u001b[39m         warnings.warn(\n\u001b[32m    904\u001b[39m             (\n\u001b[32m    905\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) has a `transform`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    914\u001b[39m         )\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    920\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:894\u001b[39m, in \u001b[36mStandardScaler.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    892\u001b[39m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    893\u001b[39m \u001b[38;5;28mself\u001b[39m._reset()\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:930\u001b[39m, in \u001b[36mStandardScaler.partial_fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[32m    899\u001b[39m \n\u001b[32m    900\u001b[39m \u001b[33;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    927\u001b[39m \u001b[33;03m    Fitted scaler.\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    929\u001b[39m first_call = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mn_samples_seen_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    938\u001b[39m n_features = X.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2942\u001b[39m         out = X, y\n\u001b[32m   2943\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2944\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2945\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2946\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:1130\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1128\u001b[39m     n_samples = _num_samples(array)\n\u001b[32m   1129\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_samples < ensure_min_samples:\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m) while a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1133\u001b[39m             % (n_samples, array.shape, ensure_min_samples, context)\n\u001b[32m   1134\u001b[39m         )\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array.ndim == \u001b[32m2\u001b[39m:\n\u001b[32m   1137\u001b[39m     n_features = array.shape[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: Found array with 0 sample(s) (shape=(0, 12)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "#F. Escalonamento / normalização\n",
    "\n",
    "# Verificar tipos de dados antes da normalização\n",
    "print(f'Tipos de dados antes da normalização:\\n{df_heart_cleaned.dtypes}')\n",
    "\n",
    "# Inicializando o StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Normalizar todas as variáveis numéricas (int e float)\n",
    "numeric_columns = df_heart_cleaned.select_dtypes(include=\"number\").columns\n",
    "df_heart_cleaned[numeric_columns] = scaler.fit_transform(df_heart_cleaned[numeric_columns])\n",
    "\n",
    "# Conversão dos valores normalizados para int\n",
    "df_heart_cleaned[numeric_columns] = df_heart_cleaned[numeric_columns].round().astype('int')\n",
    "\n",
    "print(f'Tipos de dados depois da normalização:\\n{df_heart_cleaned.dtypes}')\n",
    "\n",
    "# Verificar se as variáveis estão escalonadas\n",
    "print(df_heart_cleaned[numeric_columns].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563d503b",
   "metadata": {},
   "source": [
    "#### Exercício 1 – Deteção de Outliers Polars (IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b8cfb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de duplicados: 0\n",
      "\n",
      "Valores ausentes finais:\n",
      "age                         0\n",
      "anaemia                     0\n",
      "creatinine_phosphokinase    0\n",
      "diabetes                    0\n",
      "ejection_fraction           0\n",
      "high_blood_pressure         0\n",
      "serum_creatinine            0\n",
      "serum_sodium                0\n",
      "sex                         0\n",
      "smoking                     0\n",
      "time                        0\n",
      "DEATH_EVENT                 0\n",
      "dtype: int64\n",
      "\n",
      "Colunas numéricas escalonadas:\n",
      "Empty DataFrame\n",
      "Columns: [age, anaemia, creatinine_phosphokinase, diabetes, ejection_fraction, high_blood_pressure, serum_creatinine, serum_sodium, sex, smoking, time, DEATH_EVENT]\n",
      "Index: []\n",
      "\n",
      "Colunas categóricas codificadas:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#G. Validação final\n",
    "\n",
    "# Verificar duplicados\n",
    "duplicates = df_heart_cleaned.duplicated().sum()\n",
    "print(f'Número de duplicados: {duplicates}')\n",
    "\n",
    "# Verificar ausência de valores nulos\n",
    "print(f'\\nValores ausentes finais:\\n{df_heart_cleaned.isnull().sum()}')\n",
    "\n",
    "# Verificar colunas numéricas escalonadas\n",
    "print(f'\\nColunas numéricas escalonadas:\\n{df_heart_cleaned[numeric_columns].head()}')\n",
    "\n",
    "# Verificar colunas categóricas codificadas (One-Hot ou Label Encoding)\n",
    "print(f'\\nColunas categóricas codificadas:\\n{df_heart_cleaned.select_dtypes(include=[object]).head()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b499e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H. Exportação\n",
    "\n",
    "# Exportar para Pandas\n",
    "DATA_PATH_PANDAS = r'C:\\Users\\HP\\Desktop\\Formação\\Eisnt\\UFCD 10808 - Limpeza e transformação de dados em Python\\Avaliacao\\heart_prepared_pandas.csv'\n",
    "\n",
    "df_heart_cleaned.to_csv(DATA_PATH_PANDAS, index=False)\n",
    "\n",
    "# Exportar para Polars\n",
    "import polars as pl\n",
    "\n",
    "DATA_PATH_POLARS = r'C:\\Users\\HP\\Desktop\\Formação\\Eisnt\\UFCD 10808 - Limpeza e transformação de dados em Python\\Avaliacao\\heart_prepared_polars.csv'\n",
    "\n",
    "df_polars = pl.from_pandas(df_heart_cleaned)\n",
    "df_polars.write_csv(DATA_PATH_POLARS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
